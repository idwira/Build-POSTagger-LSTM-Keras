{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ZJEiEqZJX_Pa",
    "outputId": "cd7e729d-e391-4d3c-fc6e-d17cd7a694c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "# Membangun POS tagger dengan Bidirectional-LSTM menggunakan Keras\n",
    "# Sumber: https://nlpforhackers.io/lstm-pos-tagger-keras/\n",
    "\n",
    "import nltk\n",
    "# Download corpus 'treebank' dari NLTK (di run sekali saja)\n",
    "nltk.download('treebank')\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "# Melihat sampel dan ukuran (length) dari corpus\n",
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "N1JiFHlBX_Pg",
    "outputId": "0e9e07b3-cd8f-4dad-db0c-818704160f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
      " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
      " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
      " '.']\n",
      "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
      " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "# Restrukturisasi data. Memisahkan kata-kata (words) dari tag nya \n",
    "\n",
    "import numpy as np\n",
    " \n",
    "sentences, sentence_tags =[], [] \n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    "\n",
    "# Melihat bagaimana isi sebuah sequence\n",
    " \n",
    "print(sentences[5])\n",
    "print(sentence_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OH08KDxX_Pk"
   },
   "outputs": [],
   "source": [
    "# Memisahkan data menjadi data training dan test, sebelum train model nya\n",
    "\n",
    "# Digunakan fungsi train_test_split dari Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% data training, 20% data test\n",
    "(train_sentences, \n",
    " test_sentences, \n",
    " train_tags, \n",
    " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lokleW2uX_Pn"
   },
   "outputs": [],
   "source": [
    "# Meng-assign integer unik untuk tiap word (dan tag), agar bisa diolah oleh Keras\n",
    "\n",
    "# Kita mengkomputasi satu set words (dan tag) yang unik, lalu mengubahnya dalam satu list\n",
    "#   dan mengindeks nya dalam sebuah kamus (dictionary)\n",
    "\n",
    "# Kamus-kamus ini adalah word vocabulary dan tag vocabulary nya.\n",
    "\n",
    "# Kita akan juga menambahkan value khusus untuk padding sequence nya \n",
    "#   dan satu lagi untuk kata-kata tak dikenal (OOV - Out of Vocabulary)\n",
    "\n",
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    " \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    " \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # Value khusus digunakan untuk padding\n",
    "word2index['-OOV-'] = 1  # Value khusus digunakan untuk OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # Value khusus digunakan untuk padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "MnpccVx2X_Pq",
    "outputId": "1be635e7-3fc4-4f7d-ba0b-3a0e67f41bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5539, 9075, 8767, 6954, 7160, 3165, 2974, 27, 1907, 3950, 3310, 2666, 2697, 8244, 2347, 7289, 90, 8183, 5731, 3727, 1952, 9012, 6446, 810, 8720]\n",
      "[9311, 6205, 8151, 9941, 5297, 8877, 7901, 969, 810, 1172, 3947, 4326, 3970, 7453, 928, 1408, 8564, 362, 890, 255, 9555, 7453, 9705, 1, 7636, 7286, 7848, 1172, 1417, 6910, 6523, 1, 1, 1172, 4051, 8710, 3950, 2720, 3979, 6987, 7208, 8720]\n",
      "[18, 22, 11, 34, 15, 6, 2, 44, 28, 44, 28, 29, 29, 15, 9, 29, 11, 44, 28, 29, 20, 45, 43, 20, 37]\n",
      "[23, 31, 2, 29, 43, 44, 2, 29, 20, 8, 40, 43, 20, 10, 13, 28, 29, 20, 44, 29, 20, 10, 13, 11, 44, 22, 11, 8, 4, 43, 20, 25, 25, 8, 29, 29, 44, 28, 25, 25, 25, 37]\n"
     ]
    }
   ],
   "source": [
    "# Sekarang kita konversi dataset word ke dataset integer, untuk words nya dan tag nya\n",
    "\n",
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    " \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    " \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q7-0G639X_Ps",
    "outputId": "5ffef4c2-5f79-4345-f0b9-96e4a04109c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "# Keras hanya bisa bekerja dengan ukuran sequence yang fix\n",
    "# Kita akan pad ke kanan semua sequence dengan satu value khusus, dimana\n",
    "# 0 sebagai index dan \"-PAD-\" sebagai word/tag yang yang bersesuaian ke panjang dari sequence terpanjang di dataset\n",
    "\n",
    "# Hasil nya adalah panjang yang maksimum dari semua sequence\n",
    "\n",
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)  # 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yuUuUgBIX_Pw",
    "outputId": "2ad1b5c9-b623-4235-8374-4a9a0966c49d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5539 9075 8767 6954 7160 3165 2974   27 1907 3950 3310 2666 2697 8244\n",
      " 2347 7289   90 8183 5731 3727 1952 9012 6446  810 8720    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[9311 6205 8151 9941 5297 8877 7901  969  810 1172 3947 4326 3970 7453\n",
      "  928 1408 8564  362  890  255 9555 7453 9705    1 7636 7286 7848 1172\n",
      " 1417 6910 6523    1    1 1172 4051 8710 3950 2720 3979 6987 7208 8720\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[18 22 11 34 15  6  2 44 28 44 28 29 29 15  9 29 11 44 28 29 20 45 43 20\n",
      " 37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "[23 31  2 29 43 44  2 29 20  8 40 43 20 10 13 28 29 20 44 29 20 10 13 11\n",
      " 44 22 11  8  4 43 20 25 25  8 29 29 44 28 25 25 25 37  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Digunakan fungsi utility pad_sequences dari Keras \n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "dJvChqUnX_Pz",
    "outputId": "667dc668-5f19-4628-dac4-bc3138e08a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 271, 128)          1304448   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 271, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 271, 47)           24111     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 2,117,039\n",
      "Trainable params: 2,117,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Arsitektur Network, sekarang kita definisikan modelnya:\n",
    "\n",
    "# Kita akan membutuhkan embedding layer untuk mengkomputasi vector model (word) untuk words kita\n",
    "\n",
    "# Kita akan membutuhkan LSTM layer dengan sebuah modifier bidirectional,\n",
    "#   modifier tsb sebagai input bagi LSTM nilai (values) selanjutnya di dalam sequence\n",
    "#   bukan hanya yang sebelumnya\n",
    "\n",
    "# Kita butuh untuk men-set parameter return_sequence=True, sehingga output LSTM adalah sequence,\n",
    "#   bukan hanya value final nya\n",
    "\n",
    "# Setelah LSTM layer kita butuh Dense layer (atau layer yang fully-connected)\n",
    "#   yang memilih POS tag yang sesuai.\n",
    "\n",
    "# Karena Dense layer butuh untuk dijalankan pada setiap elemen dari sequence, maka kita\n",
    "#   perlu menambahkan modifier TimeDistributed\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRI_xyluX_P2"
   },
   "outputs": [],
   "source": [
    "# Ada satu hal lagi yang perlu dilakukan sebelum training:\n",
    "# Kita butuh untuk men-transform sequences dari tag ke sequences dari One-Hot Encoded tags\n",
    "# Berikut adalah fungsi yang melakukan hal tersebut\n",
    "\n",
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ACmFnvh4X_P5",
    "outputId": "83fb3f46-377c-4920-f4b6-4940a3356161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Inilah tampilan dari One Hot Encoded Tags \n",
    "\n",
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qAazIoMNX_P8",
    "outputId": "6a307f1a-e08c-4a1a-dc13-9fcca7047a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2504/2504 [==============================] - 28s 11ms/step - loss: 1.2715 - acc: 0.8589 - val_loss: 0.4034 - val_acc: 0.9096\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.3495 - acc: 0.9058 - val_loss: 0.3202 - val_acc: 0.9063\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.3152 - acc: 0.9121 - val_loss: 0.3060 - val_acc: 0.9180\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.3032 - acc: 0.9168 - val_loss: 0.2950 - val_acc: 0.9179\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2949 - acc: 0.9160 - val_loss: 0.2885 - val_acc: 0.9178\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2868 - acc: 0.9163 - val_loss: 0.2818 - val_acc: 0.9180\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2879 - acc: 0.9159 - val_loss: 0.3041 - val_acc: 0.9115\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 16s 6ms/step - loss: 0.2892 - acc: 0.9172 - val_loss: 0.2817 - val_acc: 0.9187\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2790 - acc: 0.9181 - val_loss: 0.2735 - val_acc: 0.9247\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2727 - acc: 0.9259 - val_loss: 0.2685 - val_acc: 0.9261\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2672 - acc: 0.9288 - val_loss: 0.2630 - val_acc: 0.9281\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2605 - acc: 0.9316 - val_loss: 0.2557 - val_acc: 0.9322\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 16s 7ms/step - loss: 0.2508 - acc: 0.9373 - val_loss: 0.2443 - val_acc: 0.9400\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2361 - acc: 0.9436 - val_loss: 0.2271 - val_acc: 0.9461\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2146 - acc: 0.9483 - val_loss: 0.2030 - val_acc: 0.9502\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.1864 - acc: 0.9530 - val_loss: 0.1748 - val_acc: 0.9539\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.1572 - acc: 0.9592 - val_loss: 0.1483 - val_acc: 0.9610\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.1299 - acc: 0.9665 - val_loss: 0.1264 - val_acc: 0.9668\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.1072 - acc: 0.9729 - val_loss: 0.1086 - val_acc: 0.9722\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 16s 7ms/step - loss: 0.0885 - acc: 0.9789 - val_loss: 0.0936 - val_acc: 0.9770\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0731 - acc: 0.9835 - val_loss: 0.0817 - val_acc: 0.9803\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0609 - acc: 0.9872 - val_loss: 0.0724 - val_acc: 0.9833\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0508 - acc: 0.9898 - val_loss: 0.0650 - val_acc: 0.9850\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0428 - acc: 0.9914 - val_loss: 0.0592 - val_acc: 0.9861\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0363 - acc: 0.9927 - val_loss: 0.0538 - val_acc: 0.9874\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0310 - acc: 0.9938 - val_loss: 0.0505 - val_acc: 0.9879\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 16s 7ms/step - loss: 0.0268 - acc: 0.9948 - val_loss: 0.0473 - val_acc: 0.9889\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0471 - val_acc: 0.9888\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0246 - acc: 0.9951 - val_loss: 0.0437 - val_acc: 0.9896\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0199 - acc: 0.9961 - val_loss: 0.0406 - val_acc: 0.9901\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0174 - acc: 0.9965 - val_loss: 0.0394 - val_acc: 0.9905\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0155 - acc: 0.9969 - val_loss: 0.0384 - val_acc: 0.9907\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0140 - acc: 0.9972 - val_loss: 0.0374 - val_acc: 0.9908\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0128 - acc: 0.9974 - val_loss: 0.0368 - val_acc: 0.9909\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0361 - val_acc: 0.9911\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0109 - acc: 0.9978 - val_loss: 0.0354 - val_acc: 0.9913\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.0351 - val_acc: 0.9914\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0094 - acc: 0.9981 - val_loss: 0.0348 - val_acc: 0.9914\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0349 - val_acc: 0.9914\n",
      "Epoch 40/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0082 - acc: 0.9984 - val_loss: 0.0345 - val_acc: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5352e29e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sekarang kita training model nya\n",
    "# Epochs 40\n",
    "\n",
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "41QOrI3mX_P_",
    "outputId": "b87a79c5-521e-4984-960d-b761e78c270f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 7s 9ms/step\n",
      "acc: 99.14087669146015\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi model pada data testing:\n",
    "\n",
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825 (hasil asli, 40 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tslOTLgRX_QC",
    "outputId": "400d8837-2828-47d9-ab60-c833b8bf0597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Catatan: hasil akurasi yang amat tinggi ini karena banyak nya padding\n",
    "#   kita kesempingkan dulu faktor tersebut, yang penting adalah tahapan nya\n",
    "\n",
    "# Sekarang kita ambil dua kalimat:\n",
    "\n",
    "test_samples = [\n",
    "    \"running is very important for me .\".split(),\n",
    "    \"I was running every day for a month .\".split()\n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "SzSMdZ6FX_QF",
    "outputId": "0b19d730-b64a-4a1e-8c53-14756a100208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2628 8331 1267 4433  890 6851 8720    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]\n",
      " [1566 5194 2628 2135 8446  890 1408  745 8720    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Sekarang transform test_samples diatas ke dalam padded sequences dari id-id nya word:\n",
    "\n",
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    " \n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "SweBK615X_QI",
    "outputId": "f1bd636a-e857-4fa3-9efd-316af2fc5005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.3265696e-02 1.7446376e-03 3.2747224e-02 ... 3.8458791e-04\n",
      "   5.2888278e-04 3.0618454e-03]\n",
      "  [9.1930146e-05 5.7931338e-04 4.8240243e-05 ... 4.8970906e-03\n",
      "   6.8551209e-03 7.4273138e-04]\n",
      "  [1.2951572e-05 4.0897508e-03 4.5213923e-03 ... 4.0296405e-03\n",
      "   7.7858378e-05 6.5452012e-04]\n",
      "  ...\n",
      "  [9.9994564e-01 2.0682047e-09 4.2286902e-10 ... 1.4072725e-10\n",
      "   2.1951270e-09 1.5315843e-08]\n",
      "  [9.9989986e-01 4.3015564e-09 5.0305715e-10 ... 2.2893756e-10\n",
      "   6.3541603e-09 3.6072109e-08]\n",
      "  [9.9980527e-01 9.4434576e-09 6.2562466e-10 ... 3.9498693e-10\n",
      "   1.9466844e-08 8.5982556e-08]]\n",
      "\n",
      " [[2.5589390e-05 2.8136175e-03 4.7953095e-06 ... 5.7543558e-04\n",
      "   9.0147287e-01 4.9354089e-04]\n",
      "  [1.2442895e-06 2.4854023e-02 1.0375743e-03 ... 1.0678200e-04\n",
      "   4.2358269e-03 8.4647720e-05]\n",
      "  [1.4789445e-04 4.8128720e-03 3.1223379e-02 ... 1.7817457e-04\n",
      "   2.3956534e-04 1.0269531e-03]\n",
      "  ...\n",
      "  [9.9994493e-01 1.9189839e-09 3.7345019e-10 ... 1.2985676e-10\n",
      "   2.2362223e-09 1.5132430e-08]\n",
      "  [9.9989748e-01 3.9912007e-09 4.4426629e-10 ... 2.1125371e-10\n",
      "   6.4731083e-09 3.5640078e-08]\n",
      "  [9.9979931e-01 8.7620835e-09 5.5250782e-10 ... 3.6447487e-10\n",
      "   1.9831186e-08 8.4952447e-08]]] (2, 271, 47)\n"
     ]
    }
   ],
   "source": [
    "# Membuat prediksi-prediksi pertama kita:\n",
    "\n",
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVPCOlSTX_QL"
   },
   "outputs": [],
   "source": [
    "# Cukup sulit untuk dibaca, kan? Kita butuh untuk melakukan “reverse” operation untuk to_categorical:\n",
    "\n",
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "U40VFm1EX_QO",
    "outputId": "6a1e865f-5d43-4759-a392-085e046e2423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NNP', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "# Dan berikut tampilan prediksi-prediksi nya:\n",
    "\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n",
    " \n",
    "# ['JJ', 'NNS', 'NN', 'NNP', 'NNP', 'NNS', '-NONE-', '-PAD-', ...\n",
    "# ['VBP', 'CD', 'JJ', 'CD', 'NNS', 'NNP', 'POS', 'NN', '-NONE-', '-PAD-', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW52FFx_X_QR"
   },
   "outputs": [],
   "source": [
    "# Hasil tampilan di atas mayoritas diisi oleh \"padding tokens\", karena itu akurasinya tinggi.\n",
    "# Sekarang kita tulis akurasi yang custom, yang tidak mengindahkan paddings:\n",
    "\n",
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "Ejw3W6iBX_QV",
    "outputId": "1b4fe43f-77b2-40f3-cc36-7853ec0879f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 271, 128)          1304448   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 271, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 271, 47)           24111     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 2,117,039\n",
      "Trainable params: 2,117,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sekarang kita train ulang, dengan menambahkan metric ignore_class_acuracy pada tahapan kompile \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GB5bM3JqX_QX",
    "outputId": "39eb4071-26a7-4fec-dc5f-191ad0154bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 1.4231 - acc: 0.8589 - ignore_accuracy: 0.0271 - val_loss: 0.3806 - val_acc: 0.9069 - val_ignore_accuracy: 0.0755\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.3390 - acc: 0.9084 - ignore_accuracy: 0.1072 - val_loss: 0.3168 - val_acc: 0.9082 - val_ignore_accuracy: 0.1074\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.3132 - acc: 0.9095 - ignore_accuracy: 0.1142 - val_loss: 0.3053 - val_acc: 0.9169 - val_ignore_accuracy: 0.1321\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.3028 - acc: 0.9169 - ignore_accuracy: 0.1339 - val_loss: 0.2970 - val_acc: 0.9185 - val_ignore_accuracy: 0.1368\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2951 - acc: 0.9173 - ignore_accuracy: 0.1354 - val_loss: 0.2900 - val_acc: 0.9185 - val_ignore_accuracy: 0.1369\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2878 - acc: 0.9173 - ignore_accuracy: 0.1358 - val_loss: 0.2831 - val_acc: 0.9189 - val_ignore_accuracy: 0.1404\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.2809 - acc: 0.9188 - ignore_accuracy: 0.1502 - val_loss: 0.2768 - val_acc: 0.9220 - val_ignore_accuracy: 0.1697\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2752 - acc: 0.9218 - ignore_accuracy: 0.1782 - val_loss: 0.2709 - val_acc: 0.9270 - val_ignore_accuracy: 0.2222\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2698 - acc: 0.9248 - ignore_accuracy: 0.2098 - val_loss: 0.2659 - val_acc: 0.9275 - val_ignore_accuracy: 0.2275\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2647 - acc: 0.9273 - ignore_accuracy: 0.2365 - val_loss: 0.2608 - val_acc: 0.9300 - val_ignore_accuracy: 0.2553\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2583 - acc: 0.9317 - ignore_accuracy: 0.2829 - val_loss: 0.2544 - val_acc: 0.9351 - val_ignore_accuracy: 0.3097\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2490 - acc: 0.9377 - ignore_accuracy: 0.3459 - val_loss: 0.2433 - val_acc: 0.9399 - val_ignore_accuracy: 0.3605\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2341 - acc: 0.9423 - ignore_accuracy: 0.3945 - val_loss: 0.2260 - val_acc: 0.9431 - val_ignore_accuracy: 0.3949\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.2123 - acc: 0.9464 - ignore_accuracy: 0.4382 - val_loss: 0.2024 - val_acc: 0.9490 - val_ignore_accuracy: 0.4579\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.1863 - acc: 0.9511 - ignore_accuracy: 0.4877 - val_loss: 0.1775 - val_acc: 0.9529 - val_ignore_accuracy: 0.5001\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 19s 8ms/step - loss: 0.1605 - acc: 0.9566 - ignore_accuracy: 0.5459 - val_loss: 0.1548 - val_acc: 0.9566 - val_ignore_accuracy: 0.5397\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.1368 - acc: 0.9632 - ignore_accuracy: 0.6144 - val_loss: 0.1350 - val_acc: 0.9636 - val_ignore_accuracy: 0.6148\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 19s 8ms/step - loss: 0.1158 - acc: 0.9698 - ignore_accuracy: 0.6839 - val_loss: 0.1169 - val_acc: 0.9693 - val_ignore_accuracy: 0.6756\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0972 - acc: 0.9761 - ignore_accuracy: 0.7500 - val_loss: 0.1020 - val_acc: 0.9749 - val_ignore_accuracy: 0.7357\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0810 - acc: 0.9819 - ignore_accuracy: 0.8107 - val_loss: 0.0886 - val_acc: 0.9788 - val_ignore_accuracy: 0.7769\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0671 - acc: 0.9862 - ignore_accuracy: 0.8557 - val_loss: 0.0780 - val_acc: 0.9823 - val_ignore_accuracy: 0.8141\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0557 - acc: 0.9891 - ignore_accuracy: 0.8862 - val_loss: 0.0682 - val_acc: 0.9848 - val_ignore_accuracy: 0.8395\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0459 - acc: 0.9912 - ignore_accuracy: 0.9078 - val_loss: 0.0606 - val_acc: 0.9861 - val_ignore_accuracy: 0.8530\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0381 - acc: 0.9926 - ignore_accuracy: 0.9229 - val_loss: 0.0548 - val_acc: 0.9874 - val_ignore_accuracy: 0.8671\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0319 - acc: 0.9938 - ignore_accuracy: 0.9358 - val_loss: 0.0506 - val_acc: 0.9884 - val_ignore_accuracy: 0.8778\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0271 - acc: 0.9948 - ignore_accuracy: 0.9461 - val_loss: 0.0470 - val_acc: 0.9891 - val_ignore_accuracy: 0.8861\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0232 - acc: 0.9957 - ignore_accuracy: 0.9549 - val_loss: 0.0441 - val_acc: 0.9896 - val_ignore_accuracy: 0.8914\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0202 - acc: 0.9961 - ignore_accuracy: 0.9599 - val_loss: 0.0417 - val_acc: 0.9902 - val_ignore_accuracy: 0.8962\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0177 - acc: 0.9967 - ignore_accuracy: 0.9651 - val_loss: 0.0405 - val_acc: 0.9904 - val_ignore_accuracy: 0.8995\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0157 - acc: 0.9969 - ignore_accuracy: 0.9681 - val_loss: 0.0390 - val_acc: 0.9907 - val_ignore_accuracy: 0.9021\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0141 - acc: 0.9972 - ignore_accuracy: 0.9710 - val_loss: 0.0384 - val_acc: 0.9908 - val_ignore_accuracy: 0.9041\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0128 - acc: 0.9975 - ignore_accuracy: 0.9738 - val_loss: 0.0375 - val_acc: 0.9908 - val_ignore_accuracy: 0.9041\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0117 - acc: 0.9977 - ignore_accuracy: 0.9755 - val_loss: 0.0371 - val_acc: 0.9910 - val_ignore_accuracy: 0.9053\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 17s 7ms/step - loss: 0.0107 - acc: 0.9979 - ignore_accuracy: 0.9780 - val_loss: 0.0362 - val_acc: 0.9912 - val_ignore_accuracy: 0.9075\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0098 - acc: 0.9980 - ignore_accuracy: 0.9795 - val_loss: 0.0361 - val_acc: 0.9912 - val_ignore_accuracy: 0.9071\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0092 - acc: 0.9981 - ignore_accuracy: 0.9805 - val_loss: 0.0367 - val_acc: 0.9911 - val_ignore_accuracy: 0.9079\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0085 - acc: 0.9983 - ignore_accuracy: 0.9824 - val_loss: 0.0365 - val_acc: 0.9911 - val_ignore_accuracy: 0.9074\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0078 - acc: 0.9985 - ignore_accuracy: 0.9839 - val_loss: 0.0365 - val_acc: 0.9911 - val_ignore_accuracy: 0.9073\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0073 - acc: 0.9985 - ignore_accuracy: 0.9848 - val_loss: 0.0356 - val_acc: 0.9912 - val_ignore_accuracy: 0.9077\n",
      "Epoch 40/40\n",
      "2504/2504 [==============================] - 18s 7ms/step - loss: 0.0069 - acc: 0.9986 - ignore_accuracy: 0.9856 - val_loss: 0.0355 - val_acc: 0.9912 - val_ignore_accuracy: 0.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4e009be80>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sekarang kita train ulang:\n",
    "\n",
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nOVlimsaX_Qa",
    "outputId": "0ae7ccf1-b7ad-47e6-e934-736b76eefcf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NNP', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "#Sekarang kita lihat bagaimana performa model nya\n",
    "predictions = model.predict(test_samples_X)\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtdR0UqYX_Qd"
   },
   "outputs": [],
   "source": [
    "# Hasilnya sudah amat baik dan ada kemungkinan bisa lebih baik lagi\n",
    "# Dengan beberapa strategi lain:\n",
    "\n",
    "# 1. Gunakan pretrained vectors – Transfer Learning\n",
    "# 2. Gunakan custom feature seperti pada POS Tagging klasik yang dikombinasikan dengan embeddings\n",
    "# 3. Coba arsitektur yang berbeda"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "33218301-SeqLabeling-DeepLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
